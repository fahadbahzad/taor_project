{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf779860-bbfc-40dd-a739-9bdfe2aae48e",
   "metadata": {},
   "source": [
    "# Julie's Notes\n",
    "\n",
    "- Do the tuples actually need quotation marks?\n",
    "    - Answer: No\n",
    "- General questions about the model\n",
    "    - Answered in person\n",
    "- Will have to change dataset so that it does not put nan as the source as this causes duplicates, not sure why they would do this\n",
    "    - Answer: We can just take the first entry of the edge tuple from the \"donor_id\" column instead of the \"source\" column\n",
    "- Had to remove any edges with NaN because it was breaking that, and we wouldn't consider that anyway.\n",
    "- It works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3ce75a-4e74-4c3e-b706-75ffebe0fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import xpress as xp\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Required inputs into the PC-TSP Algorithm:\n",
    "#      1- List of donor-receipient pairs\n",
    "#      2- List of altruisitic donors\n",
    "#      3- List of nodes = donor-receipient pairs + altruisitic donors\n",
    "#      4- Dictionary of edges with key (\"node1\",\"node2\") and value is the weight of the edge\n",
    "#      5- Dictionary of cycles with key (edge1,edge2,edge3,...) and value is the weight of the cycle i.e., \n",
    "#         sum of edges in the cycle up to and including length k\n",
    "\n",
    "\n",
    "# Mini Example\n",
    "#pairs = [\"P1\", \"P2\", \"P3\"] \n",
    "#altruistic_donors = [\"NDD1\"]\n",
    "#nodes = pairs + altruistic_donors\n",
    "#edges = {(\"NDD1\", \"P1\"): 0.10, \n",
    "#         (\"NDD1\", \"P2\"): 0.8,\n",
    "#         (\"P1\", \"P3\"): 8, \n",
    "#         (\"P2\", \"P3\"): 9,\n",
    "#         (\"P3\", \"P1\"): 6,\n",
    "#         (\"P1\", \"P2\"): 6}\n",
    "#all_cycles = {(\"P1\",\"P3\"): 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34922eb-6dbe-420b-9f14-f75fe6646ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file, turn into dataframe\n",
    "# pool = pd.read_excel('SampleData.xlsx', engine='openpyxl')\n",
    "pool = pd.read_excel('Data/Dataset1.xlsx', engine='openpyxl')\n",
    "\n",
    "# Remove unnecessary coloumns\n",
    "df = pool.drop('dage', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8363690-2202-400f-93aa-5e0fe27c48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of donor pairs\n",
    "pairs = list(set(df.loc[pd.isna(df['altruistic']), 'donor_id'].tolist()))\n",
    "\n",
    "# Create list of altruistic donors\n",
    "altruistic_donors = list(set(df.loc[df['altruistic'] == 1.0, 'donor_id'].tolist()))\n",
    "\n",
    "# Create list of nodes\n",
    "nodes = pairs + altruistic_donors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527e3162-b12c-409a-8bb8-1bfbca94038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compatibility_network(df):\n",
    "    G = nx.DiGraph() \n",
    "\n",
    "    if isinstance(df, list):\n",
    "        pool = pd.DataFrame(df)  \n",
    "    else:\n",
    "        pool = df.copy()\n",
    "    \n",
    "    # Convert to numeric and handle NaN\n",
    "    pool['donor_id'] = pd.to_numeric(pool['donor_id'], errors='coerce')\n",
    "    pool['recipient'] = pd.to_numeric(pool['recipient'], errors='coerce')\n",
    "    pool['score'] = pd.to_numeric(pool['score'], errors='coerce')\n",
    "\n",
    "    # Create a list where each donor_id maps to a list of recipients\n",
    "    matches = []\n",
    "    for _, row in pool.iterrows():\n",
    "        source = row['donor_id']\n",
    "        recipient = row['recipient']\n",
    "        score = row['score']\n",
    "\n",
    "        # Ensure valid donor & recipient\n",
    "        if not np.isnan(source) and not np.isnan(recipient):  \n",
    "            matches.append((source, recipient, score))\n",
    "    \n",
    "    # Add nodes and edges to the graph\n",
    "    for source, recipient, score in matches:\n",
    "        G.add_node(source)\n",
    "        G.add_node(recipient)\n",
    "        G.add_edge(source, recipient, score=score)\n",
    "    \n",
    "    # # Draw the graph\n",
    "    # plt.figure(figsize=(12, 10))\n",
    "    # pos = nx.spring_layout(G, k=1, seed=42) \n",
    "    # nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='black', node_size=200, font_size=5, font_weight='bold', arrows=True)\n",
    "    \n",
    "    # plt.title(\"Compatibility Network\")\n",
    "    # plt.show()\n",
    "\n",
    "    return G\n",
    "\n",
    "G = compatibility_network(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19618093-bcc5-437f-b377-4f073c41a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edges dictionary with weights\n",
    "edges = df.dropna(subset=['score']).set_index(['donor_id', 'recipient'])['score'].to_dict()\n",
    "\n",
    "# Create cycles dictionary\n",
    "cycles = sorted(nx.simple_cycles(G, length_bound=3))\n",
    "\n",
    "# Very particular output to make sure we can turn it into a dictionary\n",
    "with open(\"output.txt\", \"w\") as f: \n",
    "    for cycle in cycles:\n",
    "        print(\",\".join(map(str, cycle)), file=f)\n",
    "\n",
    "# Read the file and convert each line into a tuple\n",
    "with open(\"output.txt\", \"r\") as f:\n",
    "    cycles = [tuple(map(float, line.strip().split(\",\"))) for line in f if line.strip()]\n",
    "\n",
    "# Convert list of tuples into a dictionary (no weights yet)\n",
    "cycles_dict = {i: cycle for i, cycle in enumerate(cycles)}\n",
    "\n",
    "# Function to calculate the sum of weights for a given cycle\n",
    "def get_cycle_weight(cycle, edges):\n",
    "    total_weight = 0\n",
    "    for i in range(len(cycle) - 1):\n",
    "        edge = (cycle[i], cycle[i + 1])\n",
    "        if edge in edges:\n",
    "            total_weight += edges[edge]\n",
    "        \n",
    "    # Add the weight for the edge from the last node to the first node\n",
    "    closing_edge = (cycle[-1], cycle[0])\n",
    "    if closing_edge in edges:\n",
    "        total_weight += edges[closing_edge]\n",
    "        \n",
    "    return total_weight\n",
    "\n",
    "# Create dictionary with cycles and weights\n",
    "all_cycles = {tuple(cycle): get_cycle_weight(cycle, edges) for cycle in cycles_dict.values()}\n",
    "\n",
    "# Filter out all cycles of length 2 i.e., edges\n",
    "all_cycles = {key: value for key, value in all_cycles.items() if len(key) != 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c61ff9-7632-4001-b533-ba3d0f659193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph outside the separation algorithm and feed it in whenever we use it so that it doesn't create it every time\n",
    "# instead, it just updates the weights i.e., you only create the graph once!\n",
    "\n",
    "# Add super source 'SUPER' and connect to all NDDs\n",
    "super_source = \"SUPER\"\n",
    "G.add_node(super_source)\n",
    "for u in altruistic_donors:\n",
    "    G.add_edge(super_source, u, weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f58e4ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FICO Xpress v9.4.2, Hyper, solve started 19:48:50, Mar 10, 2025\n",
      "Heap usage: 2601KB (peak 2601KB, 274KB system)\n",
      "Maximizing MILP noname using up to 14 threads and up to 15GB memory, with these control settings:\n",
      "OUTPUTLOG = 1\n",
      "NLPPOSTSOLVE = 1\n",
      "XSLP_DELETIONCONTROL = 0\n",
      "XSLP_OBJSENSE = -1\n",
      "Original problem has:\n",
      "       916 rows         6643 cols        14330 elements      6643 entities\n",
      "Presolved problem has:\n",
      "       540 rows         6294 cols        14145 elements      6294 entities\n",
      "Presolve finished in 0 seconds\n",
      "Heap usage: 4027KB (peak 6010KB, 274KB system)\n",
      "\n",
      "Coefficient range                    original                 solved        \n",
      "  Coefficients   [min,max] : [ 1.00e+00,  1.00e+00] / [ 1.00e+00,  1.00e+00]\n",
      "  RHS and bounds [min,max] : [ 1.00e+00,  1.00e+00] / [ 1.00e+00,  1.00e+00]\n",
      "  Objective      [min,max] : [ 1.00e+00,  2.59e+02] / [ 1.00e+00,  2.59e+02]\n",
      "Autoscaling applied standard scaling\n",
      "\n",
      "Will try to keep branch and bound tree memory usage below 8.5GB\n",
      " *** Solution found:      .000000   Time:   0.06    Heuristic: T ***\n",
      " *** Solution found:  5045.000000   Time:   0.25    Heuristic: e ***\n",
      "Starting concurrent solve with dual (1 thread), primal (1 thread) and barrier (12 threads)\n",
      "\n",
      "                           Concurrent-Solve,   0s\n",
      "            Dual                      Primal                     Barrier      \n",
      "    objective   dual inf                                                      \n",
      " D  12375.000   .0000000 |                          |                         \n",
      "                         |                          |                         \n",
      "------- optimal -------- | ----- interrupted ------ | ----- interrupted ------\n",
      "Concurrent statistics:\n",
      "           Dual: 693 simplex iterations, 0.00s\n",
      "         Primal: 1084 simplex iterations, 0.00s\n",
      "        Barrier: 0 barrier and 0 simplex iterations, 0.01s\n",
      "            Barrier used 12 threads 12 cores\n",
      "            Barrier used AVX2 support\n",
      "Optimal solution found\n",
      " \n",
      "   Its         Obj Value      S   Ninf  Nneg   Sum Dual Inf  Time\n",
      "   693       12375.00000      D      0     0        .000000     0\n",
      "Dual solved problem\n",
      "  693 simplex iterations in 0.01 seconds at time 0\n",
      "\n",
      "Final objective                       : 1.237500000000000e+04\n",
      "  Max primal violation      (abs/rel) :       0.0 /       0.0\n",
      "  Max dual violation        (abs/rel) :       0.0 /       0.0\n",
      "  Max complementarity viol. (abs/rel) :       0.0 /       0.0\n",
      "\n",
      "Starting root cutting & heuristics\n",
      "Deterministic mode with up to 1 additional thread\n",
      " \n",
      " Its Type    BestSoln    BestBound   Sols    Add    Del     Gap     GInf   Time\n",
      "P         5074.000000  12375.00000      3                 59.00%       0      1\n",
      "P         5255.000000  12375.00000      4                 57.54%       0      1\n",
      "P         6775.000000  12375.00000      5                 45.25%       0      2\n",
      "Heuristic search 'R' started\n",
      "Heuristic search 'R' stopped\n",
      "M         6851.000000  12375.00000      6                 44.64%       0      4\n",
      "M         6931.000000  12375.00000      7                 43.99%       0      4\n",
      "M         7021.000000  12375.00000      8                 43.26%       0      4\n",
      "M         7108.000000  12375.00000      9                 42.56%       0      4\n",
      "M         7182.000000  12375.00000     10                 41.96%       0      5\n",
      "M         7253.000000  12375.00000     11                 41.39%       0      5\n",
      "M         7372.000000  12375.00000     12                 40.43%       0      5\n",
      "M         7478.000000  12375.00000     13                 39.57%       0      5\n",
      "M         7559.000000  12375.00000     14                 38.92%       0      5\n",
      "M         7651.000000  12375.00000     15                 38.17%       0      6\n",
      "M         7727.000000  12375.00000     16                 37.56%       0      6\n",
      "M         7804.000000  12375.00000     17                 36.94%       0      6\n",
      "M         7876.000000  12375.00000     18                 36.36%       0      6\n",
      "M         7933.000000  12375.00000     19                 35.89%       0      7\n",
      "M         8009.000000  12375.00000     20                 35.28%       0      7\n",
      "M         8108.000000  12375.00000     21                 34.48%       0      7\n",
      "M         8189.000000  12375.00000     22                 33.83%       0      7\n",
      "M         8275.000000  12375.00000     23                 33.13%       0      7\n",
      "M         8407.000000  12375.00000     24                 32.06%       0      8\n",
      "M         8468.000000  12375.00000     25                 31.57%       0      8\n",
      "M         8511.000000  12375.00000     26                 31.22%       0      8\n",
      "M         8568.000000  12375.00000     27                 30.76%       0      8\n",
      "M         8661.000000  12375.00000     28                 30.01%       0      9\n",
      "M         8712.000000  12375.00000     29                 29.60%       0      9\n",
      "M         8751.000000  12375.00000     30                 29.28%       0      9\n",
      "M         8798.000000  12375.00000     31                 28.91%       0     10\n",
      "M         8864.000000  12375.00000     32                 28.37%       0     10\n",
      "M         8939.000000  12375.00000     33                 27.77%       0     10\n",
      "M         9013.000000  12375.00000     34                 27.17%       0     10\n",
      "M         9073.000000  12375.00000     35                 26.68%       0     11\n",
      "M         9139.000000  12375.00000     36                 26.15%       0     11\n",
      "M         9205.000000  12375.00000     37                 25.62%       0     11\n",
      "M         9283.000000  12375.00000     38                 24.99%       0     11\n",
      "M         9359.000000  12375.00000     39                 24.37%       0     12\n",
      "M         9440.000000  12375.00000     40                 23.72%       0     12\n",
      "M         9553.000000  12375.00000     41                 22.80%       0     12\n",
      "M         9593.000000  12375.00000     42                 22.48%       0     13\n",
      "M         9641.000000  12375.00000     43                 22.09%       0     13\n",
      "M         9688.000000  12375.00000     44                 21.71%       0     13\n",
      "M         9758.000000  12375.00000     45                 21.15%       0     14\n",
      "M         9822.000000  12375.00000     46                 20.63%       0     14\n",
      "M         9864.000000  12375.00000     47                 20.29%       0     14\n",
      "M         9929.000000  12375.00000     48                 19.77%       0     15\n",
      "M         9975.000000  12375.00000     49                 19.39%       0     15\n",
      "M         10018.00000  12375.00000     50                 19.05%       0     15\n",
      "M         10094.00000  12375.00000     51                 18.43%       0     16\n",
      "P         10122.00000  12375.00000     52                 18.21%       0     16\n",
      "P         10535.00000  12375.00000     53                 14.87%       0     17\n",
      "P         10642.00000  12375.00000     54                 14.00%       0     17\n",
      "P         10812.00000  12375.00000     55                 12.63%       0     18\n",
      "P         10818.00000  12375.00000     56                 12.58%       0     19\n",
      "P         10933.00000  12375.00000     57                 11.65%       0     19\n",
      "P         10934.00000  12375.00000     58                 11.64%       0     20\n",
      "P         10943.00000  12375.00000     59                 11.57%       0     20\n",
      "P         10969.00000  12375.00000     60                 11.36%       0     21\n",
      "P         11001.00000  12375.00000     61                 11.10%       0     22\n",
      "P         11085.00000  12375.00000     62                 10.42%       0     22\n",
      "P         11190.00000  12375.00000     63                  9.58%       0     23\n",
      "P         11236.00000  12375.00000     64                  9.20%       0     24\n",
      "P         11248.00000  12375.00000     65                  9.11%       0     24\n",
      "P         11306.00000  12375.00000     66                  8.64%       0     25\n",
      "P         11330.00000  12375.00000     67                  8.44%       0     26\n",
      "P         11334.00000  12375.00000     68                  8.41%       0     27\n",
      "P         11423.00000  12375.00000     69                  7.69%       0     28\n",
      "P         11447.00000  12375.00000     70                  7.50%       0     29\n",
      "P         11624.00000  12375.00000     71                  6.07%       0     30\n",
      "P         11635.00000  12375.00000     72                  5.98%       0     31\n",
      "P         11698.00000  12375.00000     73                  5.47%       0     31\n",
      "P         11732.00000  12375.00000     74                  5.20%       0     32\n",
      "P         11793.00000  12375.00000     75                  4.70%       0     33\n",
      "P         11798.00000  12375.00000     76                  4.66%       0     34\n",
      "P         11842.00000  12375.00000     77                  4.31%       0     35\n",
      "P         11886.00000  12375.00000     78                  3.95%       0     36\n",
      "P         11904.00000  12375.00000     79                  3.81%       0     36\n",
      " *** Search completed ***\n",
      "Uncrunching matrix\n",
      "Final MIP objective                   : 1.190400000000000e+04\n",
      "Final MIP bound                       : 1.190400000000000e+04\n",
      "  Solution time / primaldual integral :     44.18s/ 19.420840%\n",
      "  Number of solutions found / nodes   :        79 /         1\n",
      "  Max primal violation      (abs/rel) :       0.0 /       0.0\n",
      "  Max integer violation     (abs    ) :       0.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pctstp as p\n",
    "importlib.reload(p)\n",
    "\n",
    "opt_val, selected_edges, selected_cycles, time_taken = p.pctsp(G, pairs, altruistic_donors, nodes, edges, all_cycles, noisy=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this is what makes it slow!\n",
    "print(len(all_cycles))\n",
    "print(len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f78ada-fd8e-4541-86df-e081a8121298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the callbacks and their implementation methods, it must be run before running the model\n",
    "\n",
    "def separation_algorithm(y, f_i, G=G):\n",
    "    \"\"\"\n",
    "    Implements the separation algorithm for cut-set constraints based on the supplementary information.\n",
    "    \n",
    "    >>> Inputs:\n",
    "    y:                    Dictionary where keys are edge tuples (u, v) and values are their solution values i.e., y[e] gotten via current_sol[0] from above\n",
    "    f_i:                  Dictionary where keys are nodes and values are the solver solution for f_i[v] gotten via current_sol[2] from above\n",
    "    G:                    Graph object from Networkx of the current problem.\n",
    "\n",
    "    >>> Outputs:\n",
    "    delta_minus_S:       List of edges i.e., (u,v) that are in the named set as per the paper\n",
    "    v:                   Node at which the constraint is violated for\n",
    "    \"\"\"\n",
    "    \n",
    "    # print(f\"Separation algorithm started...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Add edges with weights from y (solution values)\n",
    "    # Note: as per the documentation adding an edge that already exists updates the edge data.\n",
    "    for (u,v), sol_value in y.items():\n",
    "        G.add_edge(u, v, weight=sol_value)\n",
    "\n",
    "    # print(f\"Adding the edges from the given solution took {time.time() - start_time}\")\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    # Solve the max-flow min-cut problem for each node with f_i[v] > 0\n",
    "    for v, flow_in in f_i.items():\n",
    "        if flow_in > 0:\n",
    "            \n",
    "            # Compute min-cut using the function from networkx\n",
    "            cut_value, (Not_S, S) = nx.minimum_cut(G, super_source, v, capacity=\"weight\")\n",
    "            \n",
    "            # If the cut weight is less than flow_in, add a violated constraint\n",
    "            if cut_value < flow_in:\n",
    "\n",
    "                #print(f\"Violated constraint for node {v}: Cut separating {Not_S} and {S}\")\n",
    " \n",
    "                \n",
    "                # Define delta_minus_S in this case\n",
    "                delta_minus_S = [(u, v) for u, v in G.edges(Not_S) if v in S]\n",
    "\n",
    "                # print(f\"Solving the max-flow min-cut problem took {time.time() - start_time}\")\n",
    "\n",
    "                # Return a list that contains the violated constraint's data needed to add the cut into \n",
    "                # Xpress using the call back.\n",
    "                return(delta_minus_S, v)\n",
    "\n",
    "\n",
    "    # print(f\"Solving the max-flow min-cut problem took {time.time() - start_time}\")\n",
    "\n",
    "    # Note: This version returns the first violated constraint found; A possible extension is \n",
    "    #       to try if its better to return all violated constraints in a single potential solution. \n",
    "    return()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def separation_cbpreintsol(prob, data, soltype, cutoff):\n",
    "    \"\"\"\n",
    "    Implements the separation callback when an integer solution is found by heuristics\n",
    "    or during the branch and bound search, but before it is accepted by the optimizer.\n",
    "    \n",
    "    >>> Inputs:\n",
    "    prob:                    Xpress model object to be passed to the callback function.\n",
    "    data:                    Data we want to pass into the callback. \n",
    "    soltype:                 Type of MIP solution that has been found from 0-3 \n",
    "    cutoff:                  Current cutoff value for the solution.\n",
    "\n",
    "    >>> Outputs:\n",
    "    ifreject:                Binary; 1 if the solution should be rejected and 0 otherwise\n",
    "    newcutoff:               New cutoff value, to be used by the optimizer if the solution is accepted. \n",
    "\n",
    "    Note: The inputs and outputs are required and predetermined in this structure based on Xpress. \n",
    "    \"\"\"\n",
    "    \n",
    "    # print(\"##################### CBPREINTSOL EXECUTED ############################\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialisze outputs:\n",
    "    ifreject = 0\n",
    "    newcutoff = None # Note: this is not being used currently, we will potentially explore it in extensions\n",
    "    \n",
    "    # Get LP relaxation solution\n",
    "    lp_solution = []\n",
    "    prob.getlpsol(lp_solution, None, None, None)\n",
    "    \n",
    "    # print(f\"Getting the LP relaxation solution took {time.time() - start_time} seconds\")\n",
    "\n",
    "\n",
    "    # Populate values from lp_solution using iteration which should be faster as it is just an O(1) process.\n",
    "    # Note: we could stop at f_i_temp if it gets too slow. \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    solution_iter = iter(lp_solution)\n",
    "    \n",
    "    for key in y_temp:\n",
    "        y_temp[key] = next(solution_iter)\n",
    "    \n",
    "    for key in z_temp:\n",
    "        z_temp[key] = next(solution_iter)\n",
    "    \n",
    "    for key in f_i_temp:\n",
    "        f_i_temp[key] = next(solution_iter)\n",
    "    \n",
    "    # for key in f_o_temp:\n",
    "    #     f_o_temp[key] = next(solution_iter)\n",
    "    \n",
    "    # print(f\"Converting the solution into dictionary terms took {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Get the violated constraint data using the separation algorithm from above\n",
    "    violated_constraint_data = separation_algorithm(y_temp, f_i_temp)\n",
    "\n",
    "    # Check if there is a violated constraint\n",
    "    if violated_constraint_data:\n",
    "\n",
    "        # Reject the solution\n",
    "        ifreject = 1\n",
    "        \n",
    "        # There is a violated constraint so return 1/True i.e., reject this solution\n",
    "        # Note: From the research I've found the purpose of this callback is to just reject solutions\n",
    "        #       and not to add constraints to the problem dynamically. This is particularly relevant\n",
    "        #       for Andrew's part because he'll probably have to do it recursively outside the solver. \n",
    "            \n",
    "    # Otherwise, return 0/False i.e., this solution does not violate a constraint so its good!\n",
    "    return(ifreject, newcutoff)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def separation_cboptnode(prob, data):\n",
    "    \"\"\"\n",
    "    Implements the separation callback when called during the branch and bound search, after the LP\n",
    "    relaxation has been solved for the current node, and after any internal cuts and heuristics have been\n",
    "    applied, but before the optimizer checks if the current node should be branched. \n",
    "        \n",
    "    >>> Inputs:\n",
    "    prob:                    Xpress model object to be passed to the callback function.\n",
    "    data:                    Data we want to pass into the callback. \n",
    "    \n",
    "    >>> Outputs:\n",
    "    feas:                    Feasibility status; If set to a nonzero value the current node is set to infeasible\n",
    "\n",
    "    Note: The inputs and outputs are required and predetermined in this structure based on Xpress. \n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!! CBOPTNODE EXECUTED !!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "    \n",
    "    # Initialise output \n",
    "    feas = 0\n",
    "    \n",
    "    # Get LP relaxation solution\n",
    "    lp_solution = []\n",
    "    prob.getlpsol(lp_solution, None, None, None)\n",
    "    \n",
    "    # Populate values from lp_solution using iteration which should be faster as it is just an O(1) process.\n",
    "    # Note: we could stop at f_i_temp if it gets too slow. \n",
    "    solution_iter = iter(lp_solution)\n",
    "    \n",
    "    for key in y_temp:\n",
    "        y_temp[key] = next(solution_iter)\n",
    "    \n",
    "    for key in z_temp:\n",
    "        z_temp[key] = next(solution_iter)\n",
    "    \n",
    "    for key in f_i_temp:\n",
    "        f_i_temp[key] = next(solution_iter)\n",
    "    \n",
    "    # for key in f_o_temp:\n",
    "    #     f_o_temp[key] = next(solution_iter)\n",
    "\n",
    "    \n",
    "    # Get the violated constraint data using the separation algorithm from above\n",
    "    violated_constraint_data = separation_algorithm(y_temp, f_i_temp)\n",
    "    \n",
    "    # If there is a violated constraint add it as a cut\n",
    "    if violated_constraint_data:\n",
    "        \n",
    "        # Unpack the tuple to get the data we want\n",
    "        delta_minus_S, v = violated_constraint_data  \n",
    "\n",
    "        # Translate the cut into the presolve index\n",
    "        colind, rowcoef = [], []\n",
    "        \n",
    "        drhsp, status = prob.presolverow(      rowtype = \"G\", # 'G' for >= constraints\n",
    "                                               origcolind = [prob.getIndex(y[e]) for e in delta_minus_S] + [prob.getIndex(f_i[v])], # Index values of original variables\n",
    "                                               origrowcoef = [1] * len(delta_minus_S) + [-1], # Coefficients for original variables\n",
    "                                               origrhs = 0, # Right-hand side of constraint in original variables\n",
    "                                               maxcoefs=prob.attributes.cols, # what does this do?\n",
    "                                               colind=colind, rowcoef=rowcoef # where to output the new ones\n",
    "                                        ) \n",
    "    \n",
    "        # Now we add the translated cut\n",
    "        prob.addcuts(\n",
    "                            cuttype=[1],  # General cut\n",
    "                            rowtype=['G'],  # Presolved row type are we sure??\n",
    "                            rhs=[drhsp],  # Presolved RHS\n",
    "                            start=[0, len(colind)],  # Start indices what does this do? \n",
    "                            colind=colind,  # Presolved column indices\n",
    "                            cutcoef=rowcoef  # Presolved coefficients\n",
    "                    )\n",
    "\n",
    "    # Continue solving\n",
    "    return(feas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a848fb-9179-4698-af8f-67437ebd1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Define the optimization model\n",
    "model = xp.problem()\n",
    "\n",
    "# Decision variables\n",
    "y = {e: xp.var(vartype=xp.binary, name=f\"y_{e}\") for e in edges}  # Edge selection\n",
    "z = {c: xp.var(vartype=xp.binary, name=f\"z_{c}\") for c in all_cycles}  # Cycle selection\n",
    "f_i = {v: xp.var(vartype=xp.binary) for v in nodes}  # Flow in decision variable\n",
    "f_o = {v: xp.var(vartype=xp.binary) for v in nodes}  # Flow out decision variable\n",
    "\n",
    "# Add decision variables\n",
    "model.addVariable(list(y.values()) + list(z.values())+ list(f_i.values()) + list(f_o.values()))\n",
    "\n",
    "\n",
    "\n",
    "# Xpress uses indexing when in callback thats why we need to create a dictionary for the ids. I suspect when you run this on actual data\n",
    "# you would want to do this in another code cell just so that it isn't repeated every time you solve the model for debugging. \n",
    "# e.g., id_vars[(\"y\", ('NDD1', 'P1') )] = 0 i.e., the decision variable to connect nodes NDD1 and P1 is the first decision variable in Xpress.\n",
    "\n",
    "# Initialize id_vars dictionary and counter to assign sequential values\n",
    "id_vars = {}\n",
    "counter = 0\n",
    "\n",
    "# Populate id_vars with (\"y\", key) tuples\n",
    "for key in y.keys():\n",
    "    id_vars[(\"y\", key)] = counter\n",
    "    counter += 1\n",
    "    \n",
    "\n",
    "# Populate id_vars with (\"z\", key) tuples\n",
    "for key in z.keys():\n",
    "    id_vars[(\"z\", key)] = counter\n",
    "    counter += 1\n",
    "\n",
    "# Populate id_vars with (\"f_i\", key) tuples\n",
    "for key in f_i.keys():\n",
    "    id_vars[(\"f_i\", key)] = counter\n",
    "    counter += 1\n",
    "\n",
    "# Populate id_vars with (\"f_o\", key) tuples\n",
    "for key in f_o.keys():\n",
    "    id_vars[(\"f_o\", key)] = counter\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# Create temporary storage for the callback solutions to be used:\n",
    "y_temp = {e: 0 for e in edges}\n",
    "z_temp = {c: 0 for c in all_cycles} \n",
    "f_i_temp = {v: 0 for v in nodes}  \n",
    "f_o_temp = {v: 0 for v in nodes}  \n",
    "\n",
    "\n",
    "# Define the objective function which is to maximize sum of weights of selected edges and cycles\n",
    "model.setObjective(xp.Sum(edges[e] * y[e] for e in edges) + xp.Sum(all_cycles[c] * z[c] for c in all_cycles), sense=xp.maximize)\n",
    "\n",
    "### Constraints:\n",
    "\n",
    "# 1. Defining f_i and f_o i.e., the incoming and outgoing kidneys to a node v\n",
    "for v in nodes:\n",
    "    model.addConstraint(xp.Sum([y[e] for e in edges if e[1] == v]) == f_i[v])\n",
    "    model.addConstraint(xp.Sum([y[e] for e in edges if e[0] == v]) == f_o[v])\n",
    "\n",
    "# 2. Ensure that if a node is in an actived cycle, the associated edges that involve it are turned off\n",
    "for v in pairs:\n",
    "    model.addConstraint(f_o[v] + xp.Sum([z[c] for c in all_cycles if v in c]) <= f_i[v] + xp.Sum([z[c] for c in all_cycles if v in c]))\n",
    "    model.addConstraint(f_i[v] + xp.Sum([z[c] for c in all_cycles if v in c]) <= 1)\n",
    "\n",
    "# 3. Ensure that altruistic donors donate at most one kidney  \n",
    "for v in altruistic_donors:\n",
    "    model.addConstraint(f_o[v] <= 1)\n",
    "\n",
    "\n",
    "\n",
    "# Add the call back function to detect and add violated constraints during LP relaxation\n",
    "model.addcbpreintsol(separation_cbpreintsol, None, 3)  \n",
    "model.addcboptnode(separation_cboptnode, None, 1)\n",
    "\n",
    "model.controls.outputlog = 1 # This just makes it quiet to run\n",
    "# model.setControl(\"presolve\",0)\n",
    "\n",
    "print(f\"Setting up the optimization model took {time.time()-start_time} seconds\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Solve the model\n",
    "model.solve()\n",
    "\n",
    "print(f\"Solving the model took {time.time()-start_time} seconds to execute.\")\n",
    "\n",
    "# recursive output test:\n",
    "\n",
    "\n",
    "# Print the output\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# print(\"Optimal Matches:\")\n",
    "# for (u, v), var in y.items():\n",
    "#     if model.getSolution(var) > 0.5:\n",
    "#         print(f\"{u} donates to {v} with benefit {edges[(u,v)]}\")\n",
    "\n",
    "\n",
    "print(f\"Total Benefit: {model.getObjVal()}\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# # Get the current solution\n",
    "# current_sol = ({e: model.getSolution(y[e]) for e in edges}, {c: model.getSolution(z[c]) for c in all_cycles}, {v: model.getSolution(f_i[v]) for v in nodes})\n",
    "\n",
    "# Extract and print solution\n",
    "print(\"Objective Value:\", model.getObjVal())\n",
    "selected_edges = [e for e in edges if model.getSolution(y[e]) > 0.05]\n",
    "selected_cycles = [c for c in all_cycles if model.getSolution(z[c]) > 0.05]\n",
    "print(\"Selected Edges:\", selected_edges)\n",
    "print(\"Selected Cycles:\", selected_cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb472031",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(altruistic_donors)\n",
    "\n",
    "# for edge in selected_edges:\n",
    "#     if edge[0] in [183,177,160.0,41,14]:\n",
    "#         print(edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425547f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_network(edges, cycles):\n",
    "    G = nx.DiGraph() \n",
    "\n",
    "    # Add edges\n",
    "    for e in edges:\n",
    "        G.add_edge(e[0], e[1])\n",
    "\n",
    "    # Keep track of which edges are in cycles\n",
    "    cycle_edges = set()\n",
    "\n",
    "    # Add cycles\n",
    "    for c in cycles:\n",
    "        for i in range(len(c)):\n",
    "            u, v = c[i], c[(i + 1) % len(c)]\n",
    "            G.add_edge(u, v)\n",
    "            cycle_edges.add((u, v))\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = nx.spring_layout(G)  \n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=300)\n",
    "\n",
    "    non_cycle_edges = [e for e in G.edges() if e not in cycle_edges]\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=non_cycle_edges, edge_color='black', arrows=True)\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=list(cycle_edges), edge_color='red', width=2, arrows=True)\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title(\"Solution Network\")\n",
    "    plt.show()\n",
    "\n",
    "    return G\n",
    "\n",
    "solution_network(selected_edges, selected_cycles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xpress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
